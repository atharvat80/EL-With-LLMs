{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/normalised_titles.pkl\", \"rb\") as f:\n",
    "    normalised_title = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = {\n",
    "    'aida-b': 4485,\n",
    "    'cweb': 11116,\n",
    "    'reddit-comments': 638,\n",
    "    'reddit-posts': 704,\n",
    "    'shadowlinks-shadow': 904,\n",
    "    'shadowlinks-tail': 901,\n",
    "    'shadowlinks-top': 904,\n",
    "    'tweeki': 860,\n",
    "    'wned-wiki': 6765\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_articles(titles):\n",
    "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'search',\n",
    "        'srsearch': '',\n",
    "        'srprop': 'size',\n",
    "        'utf8': 1\n",
    "    }\n",
    "\n",
    "    matching_articles = {}\n",
    "\n",
    "    s = requests.session()\n",
    "\n",
    "    for title in tqdm(titles):\n",
    "        params['srsearch'] = title\n",
    "        response = s.get(base_url, params=params).json()\n",
    "\n",
    "        if 'query' in response and 'search' in response['query']:\n",
    "            search_results = response['query']['search']\n",
    "            if len(search_results) > 0:\n",
    "                best_match = search_results[0]\n",
    "                matching_articles[title] = best_match['title']\n",
    "        else:\n",
    "            matching_articles[title] = \"UNKNOWN\"\n",
    "            print(f\"Error retrieving search results for '{title}'\")\n",
    "\n",
    "    return matching_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(string):\n",
    "    try:\n",
    "        start = string.index(\"{\")\n",
    "        end = string.index(\"}\")\n",
    "        string = string[start:end+1]\n",
    "    except ValueError:\n",
    "        return {}\n",
    "    else:\n",
    "        return json.loads(string.replace(\"\\n\", \"\"))\n",
    "\n",
    "  \n",
    "def convert_wikipedia_urls(strings):\n",
    "    converted_strings = []\n",
    "    \n",
    "    for string in strings:\n",
    "        parsed_url = urlparse(string)\n",
    "        path = parsed_url.path.strip(\"/\")\n",
    "        title = unquote(path.split(\"/\")[-1].replace(\"_\", \" \"))\n",
    "        converted_strings.append(title)\n",
    "    \n",
    "    return converted_strings\n",
    "\n",
    "\n",
    "def get_target_and_preds(file):\n",
    "    lines = []\n",
    "    with open(file, \"r\") as infile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            line = json.loads(line)\n",
    "            try:\n",
    "                response = parse_json(line['response'])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(idx, line['response'], e)\n",
    "            else:\n",
    "                if 'candidates' in response.keys():\n",
    "                    line['candidates'] = convert_wikipedia_urls(response['candidates'])\n",
    "                    # line['candidates'] = get_matching_articles(line['candidates'])\n",
    "                else:\n",
    "                    line['candidates'] = []\n",
    "                lines.append([line['title'], line['candidates']])\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_markdown_table(results):\n",
    "    datasets = [\"aida-b\", \"tweeki\", \"reddit-posts\", \"reddit-comments\", \"wned-wiki\",\n",
    "            \"shadowlinks-tail\", \"shadowlinks-shadow\", \"shadowlinks-top\"]\n",
    "\n",
    "    res = ('| k |' + ' | '.join(datasets) + ' |' + '\\n').upper()\n",
    "    res += '|-' + '-|-'.join('-' * 3 for i in range(len(datasets) + 1)) + '-|' + '\\n'\n",
    "    for i in range(5):\n",
    "        res += f'| {i + 1}' \n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                res += ' | ' + str(results[dataset][i])\n",
    "            except KeyError:\n",
    "                res += ' | - '\n",
    "        res += ' |' + '\\n'\n",
    "\n",
    "    return res.replace(\"SHADOWLINKS\", \"SLINKS\").replace(\"COMMENTS\", \"COMM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(lines, dataset, k=5, fix=False):\n",
    "    correct = 0\n",
    "    for target, candidates in lines:\n",
    "        target = target.replace(\"_\", \" \").lower()\n",
    "        if not fix:\n",
    "            candidates = [i.replace(\"_\", \" \").lower() \n",
    "                          for i in candidates][:k]\n",
    "        else:\n",
    "            candidates = [normalised_title.get(i, \"\").replace(\"_\", \" \").lower() \n",
    "                          for i in candidates][:k]\n",
    "\n",
    "        correct += int(target in candidates)\n",
    "    return round(correct/num_lines[dataset], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| K |AIDA-B | TWEEKI | REDDIT-POSTS | REDDIT-COMM. | WNED-WIKI | SLINKS-TAIL | SLINKS-SHADOW | SLINKS-TOP |\n",
       "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
       "| 1 | 0.584 | 0.463 | 0.598 | 0.408 | 0.507 | 0.38 | 0.264 | 0.469 |\n",
       "| 2 | 0.635 | 0.513 | 0.656 | 0.459 | 0.559 | 0.408 | 0.345 | 0.565 |\n",
       "| 3 | 0.651 | 0.537 | 0.673 | 0.472 | 0.578 | 0.421 | 0.375 | 0.6 |\n",
       "| 4 | 0.661 | 0.551 | 0.678 | 0.48 | 0.593 | 0.425 | 0.392 | 0.616 |\n",
       "| 5 | 0.676 | 0.555 | 0.682 | 0.483 | 0.606 | 0.436 | 0.407 | 0.619 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = list(Path(\"parametric-gpt-3\").iterdir())\n",
    "pred_titles = []\n",
    "results = {}\n",
    "\n",
    "for file in files:\n",
    "    name = file.name.split(\"-\", 1)[-1][:-6]\n",
    "    results[name] = []\n",
    "    lines = get_target_and_preds(file)\n",
    "    for _, titles in lines:\n",
    "        pred_titles.extend(titles)\n",
    "\n",
    "    for k in range(1, 6):\n",
    "        accuracy = get_accuracy(lines, name, k)\n",
    "        results[name].append(accuracy)\n",
    "\n",
    "results = get_markdown_table(results)\n",
    "with open(\"results.md\", \"a\") as f:\n",
    "    f.write(\"# GPT-3 (Parametric)\\n\" + results + \"\\n\\n\")\n",
    "\n",
    "display(Markdown(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"normalised_titles.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(normalised_title, f)\n",
    "\n",
    "# normalised_title = get_matching_articles(list(set(pred_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| K |AIDA-B | TWEEKI | REDDIT-POSTS | REDDIT-COMM. | WNED-WIKI | SLINKS-TAIL | SLINKS-SHADOW | SLINKS-TOP |\n",
       "|-----|-----|-----|-----|-----|-----|-----|-----|-----|\n",
       "| 1 | 0.71 | 0.698 | 0.697 | 0.517 | 0.64 | 0.681 | 0.386 | 0.647 |\n",
       "| 2 | 0.76 | 0.756 | 0.744 | 0.58 | 0.693 | 0.728 | 0.487 | 0.744 |\n",
       "| 3 | 0.776 | 0.785 | 0.76 | 0.603 | 0.715 | 0.759 | 0.529 | 0.782 |\n",
       "| 4 | 0.786 | 0.801 | 0.773 | 0.619 | 0.733 | 0.768 | 0.559 | 0.796 |\n",
       "| 5 | 0.802 | 0.805 | 0.778 | 0.63 | 0.748 | 0.786 | 0.574 | 0.801 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for file in files:\n",
    "    name = file.name.split(\"-\", 1)[-1][:-6]\n",
    "    results[name] = []\n",
    "    lines = get_target_and_preds(file)\n",
    "    for _, titles in lines:\n",
    "        pred_titles.extend(titles)\n",
    "\n",
    "    for k in range(1, 6):\n",
    "        accuracy = get_accuracy(lines, name, k, fix=True)\n",
    "        results[name].append(accuracy)\n",
    "\n",
    "results = get_markdown_table(results)\n",
    "with open(\"results.md\", \"a\") as f:\n",
    "    f.write(\"# GPT-3 (Parametric, Hallucinations Fixed)\\n\" + results + \"\\n\\n\")\n",
    "\n",
    "display(Markdown(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
