{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils import get_tagged_context, get_neighboring_sentences, get_wikipedia_link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/zelda_mention_entities_counter.pickle', 'rb') as handle:\n",
    "    mention_entities_counter = pickle.load(handle)\n",
    "\n",
    "with open('../data/extracts.pkl', 'rb') as handle:\n",
    "    entity_description = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_remover = re.compile(r\"[\\W]+\")\n",
    "\n",
    "def get_candidates(mention, limit=10):\n",
    "    candidates = mention_entities_counter.get(mention)\n",
    "    if candidates is None:\n",
    "        mention = mention.replace(' ', '').lower()\n",
    "        candidates = mention_entities_counter.get(mention)\n",
    "        if candidates is None:\n",
    "            mention = punctuation_remover.sub(\"\", mention)\n",
    "            candidates = mention_entities_counter.get(mention)\n",
    "    \n",
    "    if candidates is None:\n",
    "        return\n",
    "    \n",
    "    candidates = list(candidates.items())\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    if len(candidates) > limit:\n",
    "        candidates = candidates[:limit]\n",
    "\n",
    "    candidates = [i[0] for i in candidates]\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_candidates(candidates):\n",
    "    result = \"Candidates:\"\n",
    "    template = \"https://en.wikipedia.org/wiki/%s\"\n",
    "    for idx, candidate in enumerate(candidates):\n",
    "        desc = entity_description.get(candidate.replace(\" \", \"_\"), \" \")\n",
    "        candidate = template % candidate.replace(\" \", \"_\")\n",
    "        result += f\"\\n{idx +1}. {candidate} : {desc[:150]}...\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"In the following context\n",
    "\n",
    "<context>%s</context>\n",
    "\n",
    "Determine which of the following is the referent Wikipedia article of the text inside the anchor tag. \n",
    "\n",
    "%s\n",
    "\n",
    "provide your answer as `<answer>Wikipedia URL</answer>`\"\"\"\n",
    "\n",
    "\n",
    "def disambiguate(context, mention, model=\"gpt-3.5-turbo\", temperature=0.2, max_tokens=200, top_p=0.15, debug=False):\n",
    "    # messages\n",
    "    messages = []\n",
    "\n",
    "    # get candidates\n",
    "    candidates = get_candidates(mention)\n",
    "    if candidates is None:\n",
    "        print(f\"No candidates for {mention}\")\n",
    "        return None, []\n",
    "    elif len(candidates) == 1:\n",
    "        print(\"INFO Only candidate.\")\n",
    "        return candidates[0], candidates\n",
    "        \n",
    "    messages.append({\"role\": \"user\", \"content\": prompt % (context, format_candidates(candidates))})\n",
    "\n",
    "    if debug:\n",
    "        print(messages[-1][\"content\"])\n",
    "\n",
    "    candidates = [\"https://en.wikipedia.org/wiki/\" + i.replace(\" \", \"_\") for i in candidates]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Extract answer link\n",
    "    link = get_wikipedia_link(content)\n",
    "    if link is None:\n",
    "        print(\"Content:\", content)\n",
    "        raise ValueError(\"Model did not respond with an answer\")\n",
    "    else:\n",
    "        return link, candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = list(Path(\"test-data\").iterdir())\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "file = test_files[0]\n",
    "print(file)\n",
    "\n",
    "with open(file, 'r') as handle:\n",
    "    for entry in handle:\n",
    "        entry = json.loads(entry)\n",
    "        for span, title in zip(entry[\"index\"], entry[\"wikipedia_titles\"]):\n",
    "            start, end = span\n",
    "            context = get_tagged_context(entry[\"text\"], span)\n",
    "            context = get_neighboring_sentences(context, 2)\n",
    "            lines.append((context, entry[\"text\"][start:end], title))\n",
    "\n",
    "total = len(lines)\n",
    "print(total)\n",
    "print(*lines[:5], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(\"Context:\", lines[idx][0])\n",
    "print(\"Mention:\", lines[idx][1])\n",
    "result, candidates = disambiguate(lines[idx][0], lines[idx][1], debug=True)\n",
    "result, lines[idx][-1], candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "missing = []\n",
    "outfile = \"../evaluation/el-with-ek-gpt35-zelda/responses-\" + file.name.split(\"_\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(outfile, \"a\") as f:\n",
    "    for idx, (context, mention, title) in tqdm(enumerate(lines), total=len(lines)):\n",
    "        \n",
    "        if idx < start_idx:\n",
    "            continue\n",
    "        \n",
    "        title = \"https://en.wikipedia.org/wiki/\" + title.replace(\" \", \"_\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result, candidates = disambiguate(context, mention)\n",
    "            end_time = time.time()\n",
    "        except Exception as e:\n",
    "            missing.append(idx)\n",
    "            print(\"Error getting\", idx, e)\n",
    "        else :\n",
    "            if result is not None:\n",
    "                entry = {\n",
    "                    \"context\": context, \n",
    "                    \"title\": title,\n",
    "                    \"result\": result, \n",
    "                    \"candidates\": candidates,\n",
    "                }\n",
    "\n",
    "                json.dump(entry, f)\n",
    "                f.write(\"\\n\")\n",
    "                print(\"Index  :\", idx)\n",
    "                print(\"title  :\", title)\n",
    "                print(\"result :\", result)\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                # wait = max(21 - (end_time - start_time), 0)\n",
    "                # if len(candidates) > 1:\n",
    "                #     time.sleep(wait)\n",
    "        \n",
    "        start_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get missing lines\n",
    "# with open(outfile, \"r\") as f:\n",
    "#     l2 = set()\n",
    "#     for line in tqdm(f):\n",
    "#         try:\n",
    "#             line = json.loads(line)\n",
    "#         except json.JSONDecodeError:\n",
    "#             pass\n",
    "#         else:\n",
    "#             l2.add(line[\"context\"]) \n",
    "#     missing = []\n",
    "#     for idx, i in enumerate(lines):\n",
    "#         if i[0] not in l2:\n",
    "#             missing.append(idx)\n",
    "\n",
    "# lines = [lines[i] for i in missing]\n",
    "# missing = []\n",
    "# start_idx = 0\n",
    "# len(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt4free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
