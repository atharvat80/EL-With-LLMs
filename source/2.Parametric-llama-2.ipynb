{"cells":[{"cell_type":"markdown","metadata":{},"source":["> This Notebook is meant to be run on Kaggle\n","\n","# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","!export GITHUB_ACTIONS=true\n","!pip install transformers\n","!pip install auto-gptq gdown"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:21:39.823901Z","iopub.status.busy":"2023-08-07T16:21:39.823490Z","iopub.status.idle":"2023-08-07T16:21:39.835313Z","shell.execute_reply":"2023-08-07T16:21:39.834280Z","shell.execute_reply.started":"2023-08-07T16:21:39.823864Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:21:39.843360Z","iopub.status.busy":"2023-08-07T16:21:39.842586Z","iopub.status.idle":"2023-08-07T16:21:50.912436Z","shell.execute_reply":"2023-08-07T16:21:50.911454Z","shell.execute_reply.started":"2023-08-07T16:21:39.843324Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, logging\n","from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:21:50.914568Z","iopub.status.busy":"2023-08-07T16:21:50.913735Z","iopub.status.idle":"2023-08-07T16:22:51.600006Z","shell.execute_reply":"2023-08-07T16:22:51.598968Z","shell.execute_reply.started":"2023-08-07T16:21:50.914529Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1523 [00:00<?, ?w/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","\n","model = AutoGPTQForCausalLM.from_quantized(\n","    model_name_or_path,\n","    revision=\"gptq-8bit-64g-actorder_True\",\n","    use_safetensors=True,\n","    trust_remote_code=True,\n","    device_map=\"auto\",\n","    use_triton=False,\n","    quantize_config=None\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Load Candidate List"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%capture\n","!mkdir /kaggle/temp\n","!mkdir /kaggle/temp/zelda-test\n","!gdown -q -O /kaggle/temp/zelda-test.zip --fuzzy https://drive.google.com/file/d/1Qi19SfGoztNrEx8opQFd5kM1NIFdvxob/view?usp=sharing\n","!unzip /kaggle/temp/zelda-test.zip -d /kaggle/temp/zelda-test"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:51.603890Z","iopub.status.busy":"2023-08-07T16:22:51.603515Z","iopub.status.idle":"2023-08-07T16:22:51.608734Z","shell.execute_reply":"2023-08-07T16:22:51.607575Z","shell.execute_reply.started":"2023-08-07T16:22:51.603857Z"},"trusted":true},"outputs":[],"source":["import json\n","import pickle\n","import re\n","from pathlib import Path\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:51.610744Z","iopub.status.busy":"2023-08-07T16:22:51.610339Z","iopub.status.idle":"2023-08-07T16:22:59.109204Z","shell.execute_reply":"2023-08-07T16:22:59.108034Z","shell.execute_reply.started":"2023-08-07T16:22:51.610712Z"},"trusted":true},"outputs":[],"source":["punctuation_remover = re.compile(r\"[\\W]+\")\n","mention_entities_counter = None\n","\n","# load candidate list\n","with open('/kaggle/temp/zelda-test/zelda_mention_entities_counter.pickle', 'rb') as handle:\n","    mention_entities_counter = pickle.load(handle)\n","\n","# candidate retriever\n","def get_candidates(mention, limit=None):\n","    candidates = mention_entities_counter.get(mention)\n","    if candidates is None:\n","        mention = mention.replace(' ', '').lower()\n","        candidates = mention_entities_counter.get(mention)\n","        if candidates is None:\n","            mention = punctuation_remover.sub(\"\", mention)\n","            candidates = mention_entities_counter.get(mention)\n","\n","    if candidates is None:\n","        return\n","\n","    candidates = list(candidates.items())\n","    candidates.sort(key=lambda x: x[1], reverse=True)\n","    if limit is not None and len(candidates) > limit:\n","        candidates = candidates[:limit]\n","\n","    candidates = [i[0].replace(\" \", \"_\") for i in candidates]\n","    return candidates"]},{"cell_type":"markdown","metadata":{},"source":["# Prefix Constrained BeamSearch"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:59.111461Z","iopub.status.busy":"2023-08-07T16:22:59.111014Z","iopub.status.idle":"2023-08-07T16:22:59.123543Z","shell.execute_reply":"2023-08-07T16:22:59.122178Z","shell.execute_reply.started":"2023-08-07T16:22:59.111403Z"},"trusted":true},"outputs":[],"source":["def restrict_phrases(initial_input_ids, candidates):\n","    \"\"\"Restricts the answer to a fixed set of allowed phrases\"\"\"\n","    def prefix_allowed_tokens(batch_id, input_ids):\n","        # Get the answer so far\n","        decoded = tokenizer.decode(input_ids[initial_input_ids.shape[1]:]).replace(\" \", \"\")\n","\n","        # How could we continue this into a phrase\n","        phrases = [candidate[len(decoded):] for candidate in candidates\n","                   if candidate.startswith(decoded)]\n","\n","        # What token comes next?\n","        next_tokens = []\n","        for p in phrases:\n","            if p:\n","                start_token = tokenizer.encode(p, add_special_tokens=False)\n","                # remove empty string token if first\n","                if 29871 in start_token:\n","                    start_token.remove(29871)\n","                next_tokens.append(start_token[0])\n","            # if end of phrase, add eos\n","            else:\n","                next_tokens.append(tokenizer.eos_token_id)\n","        # if no next phrases, add eos\n","        next_tokens = list(set(next_tokens)) if next_tokens else [tokenizer.eos_token_id]\n","        # print(repr(decoded),phrases, next_tokens, tokenizer.batch_decode(next_tokens))\n","        return next_tokens\n","\n","    return prefix_allowed_tokens\n","\n","\n","def generate(input_ids, **kwargs):\n","    output = model.generate(\n","        inputs = input_ids,\n","        return_dict_in_generate=True,\n","        output_scores=True,\n","        **kwargs\n","    )\n","    # print(tokenizer.decode(input_ids[0]))\n","    # print(f\"Output:\\n\" + 100 * '-')\n","    for seq, score in zip(output.sequences, output.sequences_scores):\n","        decoded = tokenizer.decode(seq[input_ids.shape[1]:], skip_special_tokens=True).replace(\" \", \"\")\n","        return (score.item(), decoded)\n","    # else:\n","    #     for seq in output.sequences:\n","    #         decoded = tokenizer.decode(seq[input_ids.shape[1]:], skip_special_tokens=True).replace(\" \", \"\")\n","    #         print(repr(decoded))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:59.127142Z","iopub.status.busy":"2023-08-07T16:22:59.125424Z","iopub.status.idle":"2023-08-07T16:22:59.143070Z","shell.execute_reply":"2023-08-07T16:22:59.142027Z","shell.execute_reply.started":"2023-08-07T16:22:59.127107Z"},"trusted":true},"outputs":[],"source":["prompt = '''<s>[INST] <<SYS>>\n","You are a helpful, respectful and honest assistant.\n","<</SYS>>\n","\n","In the following context\n","\n","%s\n","\n","What is the referent Wikipedia article of \"%s\" [/INST] https://en.wikipedia.org/wiki/'''\n","\n","def disambiguate(context, mention, num_beams=5):\n","    candidates = get_candidates(mention)\n","\n","    if candidates == []:\n","        return None, None, []\n","\n","    if len(candidates) == 1:\n","        return None, candidates[0], candidates\n","\n","    input_ids = tokenizer(prompt % (context, mention), return_tensors='pt').input_ids.cuda()\n","\n","    max_new_tokens = max([len(i) for i in tokenizer(candidates, add_special_tokens=False).input_ids])\n","\n","    score, result = generate(\n","        input_ids,\n","        num_beams = num_beams,\n","        # num_beam_groups = 2,\n","        # diversity_penalty = 0.1,\n","        do_sample = False,\n","        # temperature = 0.3,\n","        # top_p = 0.8,\n","        # top_k = 20,\n","        num_return_sequences = 1,\n","        # early_stopping = True,\n","        max_new_tokens = max_new_tokens + 1,\n","        remove_invalid_values = True,\n","        prefix_allowed_tokens_fn = restrict_phrases(input_ids, candidates),\n","    )\n","\n","    return score, result, candidates"]},{"cell_type":"markdown","metadata":{},"source":["# Load Test Data"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:59.144924Z","iopub.status.busy":"2023-08-07T16:22:59.144361Z","iopub.status.idle":"2023-08-07T16:22:59.160351Z","shell.execute_reply":"2023-08-07T16:22:59.159338Z","shell.execute_reply.started":"2023-08-07T16:22:59.144888Z"},"trusted":true},"outputs":[],"source":["def get_mention_idx(sentences):\n","    for idx, s in enumerate(sentences):\n","        if \"<a>\" in s:\n","            return idx\n","\n","def get_neighboring_sentences(text, n = 1):\n","    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split the text into sentences\n","    idx = get_mention_idx(sentences)\n","    start, end = max(idx - n, 0), idx + n + 1\n","    return \" \".join(sentences[start:end])\n","\n","def get_tagged_context(text, span):\n","    s, e = span\n","    text = text[:s] + \"<a>\" + text[s:e] + \"</a>\" + text[e:]\n","    return text"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:22:59.162170Z","iopub.status.busy":"2023-08-07T16:22:59.161736Z","iopub.status.idle":"2023-08-07T16:22:59.174358Z","shell.execute_reply":"2023-08-07T16:22:59.173327Z","shell.execute_reply.started":"2023-08-07T16:22:59.162135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0 test_cweb.jsonl\n","1 test_shadowlinks-shadow.jsonl\n","2 test_shadowlinks-tail.jsonl\n","3 test_tweeki.jsonl\n","4 test_wned-wiki.jsonl\n","5 test_reddit-comments.jsonl\n","6 test_aida-b.jsonl\n","7 test_shadowlinks-top.jsonl\n","8 test_reddit-posts.jsonl\n"]}],"source":["test_files = [f for f in Path(\"/kaggle/temp/zelda-test\").iterdir() \n","              if f.name.endswith(\"jsonl\")]\n","\n","for idx, f in enumerate(test_files):\n","    print(idx, f.name)"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:34:50.661226Z","iopub.status.busy":"2023-08-07T16:34:50.660836Z","iopub.status.idle":"2023-08-07T16:34:51.068518Z","shell.execute_reply":"2023-08-07T16:34:51.067450Z","shell.execute_reply.started":"2023-08-07T16:34:50.661192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/temp/zelda-test/test_aida-b.jsonl\n","4485\n","('SOCCER - <a>JAPAN</a> GET LUCKY WIN , CHINA IN SURPRISE DEFEAT . Nadim Ladki AL-AIN , United Arab Emirates 1996-12-06 Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .', 'JAPAN', 'Japan_national_football_team')\n","('SOCCER - JAPAN GET LUCKY WIN , <a>CHINA</a> IN SURPRISE DEFEAT . Nadim Ladki AL-AIN , United Arab Emirates 1996-12-06 Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .', 'CHINA', 'China_national_football_team')\n","('SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT . Nadim Ladki <a>AL-AIN</a> , United Arab Emirates 1996-12-06 Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan . China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .', 'AL-AIN', 'Al_Ain')\n","('SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT . Nadim Ladki AL-AIN , <a>United Arab Emirates</a> 1996-12-06 Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan . China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .', 'United Arab Emirates', 'United_Arab_Emirates')\n","('SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT . Nadim Ladki AL-AIN , United Arab Emirates 1996-12-06 <a>Japan</a> began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan . China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .', 'Japan', 'Japan_national_football_team')\n"]}],"source":["lines = []\n","file = test_files[6]\n","print(file)\n","\n","with open(file, 'r') as handle:\n","    for entry in handle:\n","        entry = json.loads(entry)\n","        for span, title in zip(entry[\"index\"], entry[\"wikipedia_titles\"]):\n","            start, end = span\n","            context = get_tagged_context(entry[\"text\"], span)\n","            context = get_neighboring_sentences(context, 2)\n","            lines.append((context, entry[\"text\"][start:end], title))\n","\n","total = len(lines)\n","print(total)\n","print(*lines[:5], sep=\"\\n\")"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:34:51.791149Z","iopub.status.busy":"2023-08-07T16:34:51.790461Z","iopub.status.idle":"2023-08-07T16:34:51.931970Z","shell.execute_reply":"2023-08-07T16:34:51.930917Z","shell.execute_reply.started":"2023-08-07T16:34:51.791115Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"231aa398c348408b862546927df091a5","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["outfile = \"/kaggle/working/responses-\" + file.name.split(\"_\")[-1]\n","\n","with open(outfile, \"r\") as f:\n","    l2 = set()\n","    for line in tqdm(f):\n","        try:\n","            line = json.loads(line)\n","        except json.JSONDecodeError:\n","            pass\n","        else:\n","            l2.add(line[\"context\"]) \n","    missing = []\n","    for idx, i in enumerate(lines):\n","        if i[0] not in l2:\n","            missing.append(idx)\n","            \n","lines = [lines[i] for i in missing]\n","len(lines)\n","\n","def shorten(context, l = 300):\n","    start, end = context.index(\"<a>\"), context.index(\"</a>\")\n","    context = context[max(0, start - l): end + l]\n","    return context\n","    \n","lines = [(shorten(context), mention, target) for (context, mention, target) in lines]"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:34:52.786983Z","iopub.status.busy":"2023-08-07T16:34:52.786241Z","iopub.status.idle":"2023-08-07T16:34:52.792352Z","shell.execute_reply":"2023-08-07T16:34:52.791307Z","shell.execute_reply.started":"2023-08-07T16:34:52.786948Z"},"trusted":true},"outputs":[],"source":["start_idx = 0\n","missing = []\n","outfile = \"/kaggle/working/responses-2-\" + file.name.split(\"_\")[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T16:34:58.240507Z","iopub.status.busy":"2023-08-07T16:34:58.240091Z","iopub.status.idle":"2023-08-07T17:07:31.254452Z","shell.execute_reply":"2023-08-07T17:07:31.246422Z","shell.execute_reply.started":"2023-08-07T16:34:58.240475Z"},"trusted":true},"outputs":[],"source":["with open(outfile, \"a\") as f:\n","    for idx, (context, mention, title) in tqdm(enumerate(lines), total=len(lines)):\n","\n","        if idx < start_idx:\n","            continue\n","\n","        title = \"https://en.wikipedia.org/wiki/\" + title.replace(\" \", \"_\")\n","\n","        try:\n","            score, result, candidates = disambiguate(context, mention, num_beams=5)\n","            if result not in candidates:\n","                result = None\n","        except Exception as e:\n","            missing.append(idx)\n","            print(\"Error getting\", idx, e)\n","        else :\n","            if result is not None:\n","                entry = {\n","                    \"context\": context,\n","                    \"title\": title,\n","                    \"result\": \"https://en.wikipedia.org/wiki/\" + result,\n","                    \"candidates\": candidates,\n","                }\n","\n","                json.dump(entry, f)\n","                f.write(\"\\n\")\n","                print(\"Index  :\", idx)\n","                print(\"title  :\", title)\n","                print(\"result :\", \"https://en.wikipedia.org/wiki/\" + result)\n","                print(\"N-Cands:\", len(candidates))\n","        \n","        print(\"-\" * 50)\n","        \n","        start_idx += 1"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
